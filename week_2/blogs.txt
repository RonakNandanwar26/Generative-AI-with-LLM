Multi-task, instruction fine-tuning
	Scaling Instruction-Finetuned Language Models : https://arxiv.org/pdf/2210.11416.pdf
	Introducing FLAN: More generalizable Language Models with Instruction Fine-Tuning : https://blog.research.google/2021/10/introducing-flan-more-generalizable.html

Model Evaluation Metrics
	HELM - Holistic Evaluation of Language Models : https://crfm.stanford.edu/helm/latest/
	General Language Understanding Evaluation (GLUE) benchmark : https://openreview.net/pdf?id=rJ4km2R5t7
	SuperGLUE : https://super.gluebenchmark.com/
	ROUGE: A Package for Automatic Evaluation of Summaries : https://aclanthology.org/W04-1013.pdf
	Measuring Massive Multitask Language Understanding (MMLU) : https://arxiv.org/pdf/2009.03300.pdf
	BigBench-Hard - Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models : https://arxiv.org/pdf/2206.04615.pdf

Parameter- efficient fine tuning (PEFT)
	Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning : https://arxiv.org/pdf/2303.15647.pdf
	On the Effectiveness of Parameter-Efficient Fine-Tuning : https://arxiv.org/pdf/2211.15583.pdf

LoRA
	LoRA Low-Rank Adaptation of Large Language Models : https://arxiv.org/pdf/2106.09685.pdf
	QLoRA: Efficient Finetuning of Quantized LLMs : https://arxiv.org/pdf/2305.14314.pdf

Prompt tuning with soft prompts
	The Power of Scale for Parameter-Efficient Prompt Tuning : https://arxiv.org/pdf/2104.08691.pdf

